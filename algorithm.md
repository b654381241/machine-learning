# 机器学习Algorithm

## 聚类

### kmeans

K-Means算法是一种无监督分类算法，假设有无标签数据集：

![X= \left[ \begin{matrix} x^{(1)} \\ x^{(2)} \\ \vdots \\ x^{(m)} \\ \end{matrix} \right]](https://math.jianshu.com/math?formula=X%3D%20%5Cleft%5B%20%5Cbegin%7Bmatrix%7D%20x%5E%7B(1)%7D%20%5C%5C%20x%5E%7B(2)%7D%20%5C%5C%20%5Cvdots%20%5C%5C%20x%5E%7B(m)%7D%20%5C%5C%20%5Cend%7Bmatrix%7D%20%5Cright%5D)

该算法的任务是将数据集聚类成![k](https://math.jianshu.com/math?formula=k)个簇![C={C_{1},C_{2},...,C_{k}}](https://math.jianshu.com/math?formula=C%3D%7BC_%7B1%7D%2CC_%7B2%7D%2C...%2CC_%7Bk%7D%7D)，最小化损失函数为：

![E=\sum_{i=1}^{k}\sum_{x\in{C_{i}}}||x-\mu_{i}||^{2}](https://math.jianshu.com/math?formula=E%3D%5Csum_%7Bi%3D1%7D%5E%7Bk%7D%5Csum_%7Bx%5Cin%7BC_%7Bi%7D%7D%7D%7C%7Cx-%5Cmu_%7Bi%7D%7C%7C%5E%7B2%7D)

其中![\mu_{i}](https://math.jianshu.com/math?formula=%5Cmu_%7Bi%7D)为簇![C_{i}](https://math.jianshu.com/math?formula=C_%7Bi%7D)的中心点：

![\mu_{i}=\frac{1}{|C_{i}|}\sum_{x\in{C{i}}}x](https://math.jianshu.com/math?formula=%5Cmu_%7Bi%7D%3D%5Cfrac%7B1%7D%7B%7CC_%7Bi%7D%7C%7D%5Csum_%7Bx%5Cin%7BC%7Bi%7D%7D%7Dx)

要找到以上问题的最优解需要遍历所有可能的簇划分，K-Mmeans算法使用贪心策略求得一个近似解，具体步骤如下：

1. 在样本中随机选取![k](https://math.jianshu.com/math?formula=k)个样本点充当各个簇的中心点![\{\mu_{1},\mu_{2},...,\mu_{k}\}](https://math.jianshu.com/math?formula=%5C%7B%5Cmu_%7B1%7D%2C%5Cmu_%7B2%7D%2C...%2C%5Cmu_%7Bk%7D%5C%7D) 

2. 计算所有样本点与各个簇中心之间的距离![dist(x^{(i)},\mu_{j})](https://math.jianshu.com/math?formula=dist(x%5E%7B(i)%7D%2C%5Cmu_%7Bj%7D))，然后把样本点划入最近的簇中![x^{(i)}\in{\mu_{nearest}}](https://math.jianshu.com/math?formula=x%5E%7B(i)%7D%5Cin%7B%5Cmu_%7Bnearest%7D%7D) 

3. 根据簇中已有的样本点，重新计算簇中心
    ![\mu_{i}:=\frac{1}{|C_{i}|}\sum_{x\in{C{i}}}x](https://math.jianshu.com/math?formula=%5Cmu_%7Bi%7D%3A%3D%5Cfrac%7B1%7D%7B%7CC_%7Bi%7D%7C%7D%5Csum_%7Bx%5Cin%7BC%7Bi%7D%7D%7Dx) 

4. 重复2、3

### **改进一——kmean++**

![img](https://images2018.cnblogs.com/blog/1366181/201804/1366181-20180402200209017-1976662980.png)

### **改进二——Kernel K-means**

​    设数据集![img](http://img.blog.csdn.net/20140616121824875)，其中![img](http://img.blog.csdn.net/20140616121842109)，![img](http://img.blog.csdn.net/20140616121849937)。Mercer核函数![img](http://img.blog.csdn.net/20140616122357328)，根据Mercer定理存在映射![img](http://img.blog.csdn.net/20140616122431781)，使得![img](http://img.blog.csdn.net/20140616122453234)。

​        核K-均值聚类就是讨论映射数据集![img](http://img.blog.csdn.net/20140616122556390)在![img](http://img.blog.csdn.net/20140616122708109)空间中的聚类情况，设在![img](http://img.blog.csdn.net/20140616122708109)空间中，把数据集分为![img](http://img.blog.csdn.net/20140616122826578)类，![img](http://img.blog.csdn.net/20140616122922125)为第![img](http://img.blog.csdn.net/20140616122826578)类的均值，![img](http://img.blog.csdn.net/20140616123005000)。

即考虑以下模型：

![img](http://img.blog.csdn.net/20140616123528984)

![img](http://img.blog.csdn.net/20140616123542984)。



**问题1：**

怎么训练上述模型，因为![img](http://img.blog.csdn.net/20140616123740078)一般情况下是解不出来的。

方法：

初始化![img](http://img.blog.csdn.net/20140616123859968)，![img](http://img.blog.csdn.net/20140616123911593)，![img](http://img.blog.csdn.net/20140616123005000)，其中![img](http://img.blog.csdn.net/20140616133046875)，令

![img](http://img.blog.csdn.net/20140616133250859)，![img](http://img.blog.csdn.net/20140616123005000)。

**E步**：求![img](http://img.blog.csdn.net/20140616140421921)，

![img](http://img.blog.csdn.net/20140616140449843)

注意其中：

![img](http://img.blog.csdn.net/20140616142438250)，![img](http://img.blog.csdn.net/20140616142529593)。

**M步：**固定![img](http://img.blog.csdn.net/20140616140421921)，求![img](http://img.blog.csdn.net/20140616142958078)。

![img](http://img.blog.csdn.net/20140616143531703)，

![img](http://img.blog.csdn.net/20140616143837296),

![img](http://img.blog.csdn.net/20140616143848421)，

其中**，![img](http://img.blog.csdn.net/20140616123005000)。**

进入下一轮迭代，直至收敛！

### 改进三——ISODATA算法
​      **[1] 预期的聚类中心数目Ko**：虽然在ISODATA运行过程中聚类中心数目是可变的，但还是需要由用户指定一个参考标准。事实上，该算法的聚类中心数目变动范围也由**Ko**决定。具体地，最终输出的聚类中心数目范围是 [**Ko/2**, ***2Ko***]。

​       **[2] 每个类所要求的最少样本数目Nmin**：用于判断当某个类别所包含样本分散程度较大时是否可以进行分裂操作。如果分裂后会导致某个子类别所包含样本数目小于***Nmin***，就不会对该类别进行分裂操作。

​      **[3] 最大方差Sigma**：用于衡量某个类别中样本的分散程度。当样本的分散程度超过这个值时，则有可能进行分裂操作（注意同时需要满足**[2]**中所述的条件）。

​      **[4] 两个类别对应聚类中心之间所允许最小距离dmin**：如果两个类别靠得非常近（即这两个类别对应聚类中心之间的距离非常小），则需要对这两个类别进行合并操作。是否进行合并的阈值就是由***dmin***决定。

​      相信很多人看完上述输入的介绍后对ISODATA算法的流程已经有所猜测了。的确，ISODATA算法的原理非常直观，不过由于它和其他两个方法相比需要额外指定较多的参数，并且某些参数同样很难准确指定出一个较合理的值，因此ISODATA算法在实际过程中并没有K-means++受欢迎。

​      首先给出ISODATA算法主体部分的描述，如下图所示：

[![图4](https://images2015.cnblogs.com/blog/1024143/201701/1024143-20170111025949447-680611657.png)](http://images2015.cnblogs.com/blog/1024143/201701/1024143-20170111025947447-390971451.png)

**图4. ISODATA算法的主体部分**

​     上面描述中没有说明清楚的是第5步中的分裂操作和第6步中的合并操作。下面首先介绍合并操作：

[![图5](https://images2015.cnblogs.com/blog/1024143/201701/1024143-20170111025951775-1194408309.png)](http://images2015.cnblogs.com/blog/1024143/201701/1024143-20170111025950760-1467458924.png)

**图5. ISODATA算法的合并操作**

​     最后是ISODATA算法中的分裂操作。

[![图6](https://images2015.cnblogs.com/blog/1024143/201701/1024143-20170111025954494-895315300.png)](http://images2015.cnblogs.com/blog/1024143/201701/1024143-20170111025953056-677584793.png)

**图6. ISODATA算法的分裂操作**

​      最后，针对ISODATA算法总结一下：**该算法能够在聚类过程中根据各个类所包含样本的实际情况动态调整聚类中心的数目。如果某个类中样本分散程度较大（通过方差进行衡量）并且样本数量较大，则对其进行分裂操作；如果某两个类别靠得比较近（通过聚类中心的距离衡量），则对它们进行合并操作。**

​       可能没有表述清楚的地方是ISODATA-分裂操作的第1步和第2步。同样地以图三所示数据集为例，假设最初1，2，3，4，5，6，8号被分到了同一个类中，执行第1步和第2步结果如下所示：

[![图7](https://images2015.cnblogs.com/blog/1024143/201701/1024143-20170111025955213-1053926739.png)](http://images2015.cnblogs.com/blog/1024143/201701/1024143-20170111025954853-32277500.png)

​      而在正确分类情况下（即1，2，3，4为一类；5，6，7，8为一类），方差为0.33。因此，目前的方差远大于理想的方差，ISODATA算法就很有可能对其进行分裂操作。

### 改进对比

  (1) **K-means与K-means++：**原始K-means算法最开始随机选取数据集中K个点作为聚类中心，而K-means++按照如下的思想选取K个聚类中心：假设已经选取了n个初始聚类中心(0<n<K)，则在选取第n+1个聚类中心时：距离当前n个聚类中心越远的点会有更高的概率被选为第n+1个聚类中心。在选取第一个聚类中心(n=1)时同样通过随机的方法。可以说这也符合我们的直觉：聚类中心当然是互相离得越远越好。这个改进虽然直观简单，但是却非常得有效。

​      (2) **K-means与ISODATA：**ISODATA的全称是迭代自组织数据分析法。在K-means中，K的值需要预先人为地确定，并且在整个算法过程中无法更改。而当遇到高维度、海量的数据集时，人们往往很难准确地估计出K的大小。ISODATA就是针对这个问题进行了改进，它的思想也很直观：当属于某个类别的样本数过少时把这个类别去除，当属于某个类别的样本数过多、分散程度较大时把这个类别分为两个子类别。

​      (3) **K-means与Kernel K-means：**传统K-means采用欧式距离进行样本间的相似度度量，显然并不是所有的数据集都适用于这种度量方式。参照支持向量机中核函数的思想，将所有样本映射到另外一个特征空间中再进行聚类，就有可能改善聚类效果。

### **Mean-Shift 聚类**

1. 为了解释 mean-shift，我们将考虑一个二维空间中的点集，像上图所示那样。我们以一个圆心在C点（随机选择）的圆形滑窗开始，以半径 r 作为核。Mean shift 是一个爬山算法，它每一步都迭代地把核移动到更高密度的区域，直到收敛位置。
2. 在每次迭代时，通过移动中心点到滑窗中点的均值处，将滑窗移动到密度更高的区域（这也是这种算法名字的由来）。滑窗内的密度与在其内部点的数量成正比。很自然地，通过将中心移动到窗内点的均值处，可以逐步的移向有个高的密度的区域。
3. 我们继续根据均值来移动滑窗，直到有没有哪个方向可以使核中容纳更多的点。查看上面的图，我们一直移动圆圈直到密度不再增长。（即窗内点的数量不再增长）。
4. 用很多滑窗重复1-3这个过程，直到所有的点都包含在了窗内。当多个滑动窗口重叠时，包含最多点的窗口将被保留。然后，根据数据点所在的滑动窗口对数据点进行聚类。

下图展示了所有滑动窗口从端到端的整个过程。每个黑色的点都代表滑窗的质心，每个灰色的点都是数据点。

![img](https://hiphotos.baidu.com/feed/pic/item/f7246b600c3387447066b8b65c0fd9f9d62aa05d.jpg)

Mean-Shift 聚类的全部过程

与 K-means 聚类不同的是，Mean-Shift 不需要选择聚类的数量，因为mean-shift 自动发现它。这是一个很大的优点。事实上聚类中心向着有最大密度的点收敛也是我们非常想要的，因为这很容易理解并且很适合于自然的数据驱动的场景。缺点是滑窗尺寸/半径“r“的选择需要仔细考虑。

### DBSCAN

1. DBSCAN 从一个任意的还没有被访问过的启动数据点开始。用一个距离 epsilon ε 将这个点的邻域提取出来（所有再距离 ε 内的点都视为邻居点）。

2. 如果在邻域内有足够数量的点（根据 minPoints) ，那么聚类过程开始，并且当前数据点变成新集群中的第一个点。否则，该点将被标记为噪声（之后这个噪声点可能会变成集群中的一部分）。在这两种情况中的点都被标记为”已访问“。

3. 对于这个新集群中的第一个点，在它 ε 距离邻域内的点已将变成相同集群中的一部分。这个让所有在 ε 邻域内的点都属于相同集群的过程在之后会一直被重复做，直到所有新点都被加进集群分组中。

4. 第 2，3 步的过程会一直重复直到集群内所有点都被确定，即所有在 ε 邻域内的点都被访问且被打上标签。

5. 一旦我们在当前集群做完这些，一个新的未被访问的点会被提取并处理，从而会接着发现下一个集群或噪声。这个过程反复进行直到所有的点都被编辑为已访问。既然在最后所有的点都被访问，那么每个点都被标记为属于一个集群或者是噪声。

#### 优缺点

相较于其他聚类算法，DBSCAN 提出了一些很棒的优点。首先，它根本不需要预置集群的数量。它还将离群值认定为噪声，不像 mean-shift 中仅仅是将它们扔到一个集群里，甚至即使该数据点的差异性很大也这么做。另外，这个算法还可以很好的找到任意尺寸核任意形状的集群。

DBSCAN 最大的缺点是当集群的密度变化时，它表现的不像其他算法那样好。这是因为当密度变化时，距离的阈值 ε 和用于确定邻居点的 minPoints 也将会随之改变。这个缺点也会发生在很高为的数据中，因为距离阈值 ε 变得很难被估计。

### GMM

####一、GMM概述

![img](https://img2018.cnblogs.com/blog/1027447/201809/1027447-20180914192745456-2065802777.png)



#### 二、GMM算法步骤

![img](https://img2018.cnblogs.com/blog/1027447/201809/1027447-20180914192859284-1014242477.png)

#### 三、总结

1. GMM算法中间参数估计部分用到了EM算法，EM算法分为两步：

​      （1）E步：求目标函数期望，更多的是求目标函数取对数之后的期望值。

​      （2）M步：使期望最大化。用到极大似然估计，拉格朗日乘数法，对参数求偏导，最终确定新的参数。

2. K-means，FCM与GMM算法参数估计的数学推导思路大体一致，都先确立目标函数，然后使目标函数最大化的参数取值就是迭代公式。

3. 三个算法都需要事先指定k。K-means与FCM中的k指的是要聚的类的个数，GMM算法中的k指的是k个单高斯混合模型。

4. 三个算法流程一致：

​    （1）通过一定的方法初始化参数（eg:随机，均值······）

​    （2）确立目标函数

​    （3）通过一定的方法使目标函数最大化，更新参数迭代公式（eg:EM，粒子群······）

​    （4）设置一定的终止条件，使算法终止。若不满足条件，转向（3）

### 层次聚类

层次聚类算法分为两类：自上而下和自下而上。凝聚层级聚类(HAC)是自下而上的一种聚类算法。HAC首先将每个数据点视为一个单一的簇，然后计算所有簇之间的距离来合并簇，知道所有的簇聚合成为一个簇为止。
下图为凝聚层级聚类的一个实例：

![è¿éåå¾çæè¿°](https://img-blog.csdn.net/20180301171047257?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvS2F0aGVyaW5lX2hzcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

具体步骤：

1. 首先我们将每个数据点视为一个单一的簇，然后选择一个测量两个簇之间距离的度量标准。例如我们使用average linkage作为标准，它将两个簇之间的距离定义为第一个簇中的数据点与第二个簇中的数据点之间的平均距离。

2. 在每次迭代中，我们将两个具有最小average linkage的簇合并成为一个簇。

3. 重复步骤2知道所有的数据点合并成一个簇，然后选择我们需要多少个簇。

层次聚类优点：（1）不需要知道有多少个簇 （2）对于距离度量标准的选择并不敏感

## 插值拟合
## 数据预测


## 分类

## 异常检测

## 跟踪

## 滤波

## 控制

## 关联

## 推荐